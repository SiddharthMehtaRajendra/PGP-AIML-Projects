{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "NFfDTfhlaEI_"
   },
   "source": [
    "# Transfer Learning MNIST"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "rNwbqCFRaEJC"
   },
   "source": [
    "* Train a simple convnet on the MNIST dataset the first 5 digits [0..4].\n",
    "* Freeze convolutional layers and fine-tune dense layers for the classification of digits [5..9]."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "YUB1uDW_8XIy"
   },
   "source": [
    "## 1. Import necessary libraries for the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Rsj4t5HTaEJE"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras import applications\n",
    "from keras.models import Sequential, Model \n",
    "from keras import backend as k \n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "from keras.layers import Conv2D, MaxPooling2D, Activation, Flatten, Dense, Dropout\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "IXrn3heBaEJa"
   },
   "source": [
    "## 2. Import MNIST data and create 2 datasets with one dataset having digits from 0 to 4 and other from 5 to 9 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "pjDuiK6ztgOK"
   },
   "outputs": [],
   "source": [
    "from keras.datasets import mnist\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "# create two datasets one with digits from 0 to 4 and one with 5 to 9\n",
    "x_train_lt5 = x_train[y_train < 5]\n",
    "y_train_lt5 = y_train[y_train < 5]\n",
    "x_test_lt5 = x_test[y_test < 5]\n",
    "y_test_lt5 = y_test[y_test < 5]\n",
    "\n",
    "x_train_gte5 = x_train[y_train >= 5]\n",
    "y_train_gte5 = y_train[y_train >= 5]\n",
    "x_test_gte5 = x_test[y_test >= 5]\n",
    "y_test_gte5 = y_test[y_test >= 5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "9qU14lYL9A5g"
   },
   "source": [
    "## 3. Print x_train, y_train, x_test and y_test for both the datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Z9OrszhJ0SgJ"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset Samples: \n",
      "\n",
      "X Train < 5:  [ 51 159 253 ... 168 108  15]\n",
      "X Test < 5:  [116 125 171 ... 255 230  38]\n",
      "Y Train < 5:  0\n",
      "Y Test < 5:  2\n",
      "X Train >= 5:  [  3  18  18 ... 193 197 134]\n",
      "X Test >=5 :  [ 84 185 159 ... 132 110   4]\n",
      "Y Train >= 5:  5\n",
      "Y Test >= 5:  7\n"
     ]
    }
   ],
   "source": [
    "print(\"Dataset Samples: \\n\")\n",
    "print(\"X Train < 5: \", x_train_lt5[x_train_lt5 > 0])\n",
    "print(\"X Test < 5: \", x_test_lt5[x_test_lt5 > 0])\n",
    "print(\"Y Train < 5: \", y_train_lt5[0])\n",
    "print(\"Y Test < 5: \", y_test_lt5[0])\n",
    "print(\"X Train >= 5: \", x_train_gte5[x_train_gte5 > 0])\n",
    "print(\"X Test >=5 : \", x_test_gte5[x_test_gte5 > 0])\n",
    "print(\"Y Train >= 5: \", y_train_gte5[0])\n",
    "print(\"Y Test >= 5: \", y_test_gte5[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "cB9BPFzr9oDF"
   },
   "source": [
    "## ** 4. Let us take only the dataset (x_train, y_train, x_test, y_test) for Integers 0 to 4 in MNIST **\n",
    "## Reshape x_train and x_test to a 4 Dimensional array (channel = 1) to pass it into a Conv2D layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "FlQRPfFzaEJx"
   },
   "outputs": [],
   "source": [
    "x_train_lt5_4d = np.expand_dims(x_train_lt5, axis = 3)\n",
    "x_test_lt5_4d = np.expand_dims(x_test_lt5, axis = 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New Shape X Train:  (30596, 28, 28, 1)\n",
      "New Shape X Test:  (5139, 28, 28, 1)\n"
     ]
    }
   ],
   "source": [
    "print(\"New Shape X Train: \", x_train_lt5_4d.shape);\n",
    "print(\"New Shape X Test: \", x_test_lt5_4d.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "jLQr-b3F-hw8"
   },
   "source": [
    "## 5. Normalize x_train and x_test by dividing it by 255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "PlEZIAG5-g2I"
   },
   "outputs": [],
   "source": [
    "x_train_lt5_4d = x_train_lt5_4d/255;\n",
    "x_test_lt5_4d = x_test_lt5_4d/255"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "pytVBaw4-vMi"
   },
   "source": [
    "## 6. Use One-hot encoding to divide y_train and y_test into required no of output classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "V48xiua4-uUi"
   },
   "outputs": [],
   "source": [
    "y_train_enc = pd.get_dummies(y_train_lt5)\n",
    "y_test_enc = pd.get_dummies(y_test_lt5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_lt5 = y_train_enc\n",
    "y_test_lt5 = y_test_enc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "elPkI44g_C2b"
   },
   "source": [
    "## 7. Build a sequential model with 2 Convolutional layers with 32 kernels of size (3,3) followed by a Max pooling layer of size (2,2) followed by a drop out layer to be trained for classification of digits 0-4  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "MU09mm9F89gO"
   },
   "outputs": [],
   "source": [
    "# input image dimensions\n",
    "img_rows, img_cols = 28, 28\n",
    "# number of convolutional filters to use\n",
    "filters = 32\n",
    "# size of pooling area for max pooling\n",
    "pool_size = 2\n",
    "# convolution kernel size\n",
    "kernel_size = 3\n",
    "# number of classes\n",
    "num_classes = 5\n",
    "\n",
    "conv_layers = [\n",
    "    Conv2D(filters, kernel_size,\n",
    "           padding='valid',\n",
    "           input_shape=(28, 28, 1)),\n",
    "    Activation('relu'),\n",
    "    Conv2D(filters, kernel_size),\n",
    "    Activation('relu'),\n",
    "    MaxPooling2D(pool_size = pool_size),\n",
    "    Dropout(0.25),\n",
    "    Flatten(),\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "sJQaycRO_3Au"
   },
   "source": [
    "## 8. Post that flatten the data and add 2 Dense layers with 128 neurons and neurons = output classes with activation = 'relu' and 'softmax' respectively. Add dropout layer inbetween if necessary  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "vOZeRbK7t9AT"
   },
   "outputs": [],
   "source": [
    "#Referenced from a similar GIT Project\n",
    "output_layers = [\n",
    "    Dense(128),\n",
    "    Activation('relu'),\n",
    "    Dropout(0.5),\n",
    "    Dense(num_classes),\n",
    "    Activation('softmax')\n",
    "]\n",
    "\n",
    "# create complete model\n",
    "model = Sequential(conv_layers + output_layers)\n",
    "\n",
    "# Save the model \n",
    "checkpoint = ModelCheckpoint(\"vgg16_initial_best.h5\", monitor='val_acc', verbose=1, save_best_only=True, mode='auto')\n",
    "early = EarlyStopping(monitor='val_acc', min_delta=0, patience=5, verbose=1, mode='auto')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_3 (Conv2D)            (None, 26, 26, 32)        320       \n",
      "_________________________________________________________________\n",
      "activation_5 (Activation)    (None, 26, 26, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_4 (Conv2D)            (None, 24, 24, 32)        9248      \n",
      "_________________________________________________________________\n",
      "activation_6 (Activation)    (None, 24, 24, 32)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 12, 12, 32)        0         \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 12, 12, 32)        0         \n",
      "_________________________________________________________________\n",
      "flatten_2 (Flatten)          (None, 4608)              0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 128)               589952    \n",
      "_________________________________________________________________\n",
      "activation_7 (Activation)    (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 5)                 645       \n",
      "_________________________________________________________________\n",
      "activation_8 (Activation)    (None, 5)                 0         \n",
      "=================================================================\n",
      "Total params: 600,165\n",
      "Trainable params: 600,165\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 30596 samples, validate on 5139 samples\n",
      "Epoch 1/10\n",
      "30596/30596 [==============================] - 3s 84us/step - loss: 0.3058 - acc: 0.9041 - val_loss: 0.0557 - val_acc: 0.9829\n",
      "\n",
      "Epoch 00001: val_acc improved from -inf to 0.98288, saving model to vgg16_initial_best.h5\n",
      "Epoch 2/10\n",
      "30596/30596 [==============================] - 2s 64us/step - loss: 0.0739 - acc: 0.9786 - val_loss: 0.0255 - val_acc: 0.9922\n",
      "\n",
      "Epoch 00002: val_acc improved from 0.98288 to 0.99222, saving model to vgg16_initial_best.h5\n",
      "Epoch 3/10\n",
      "30596/30596 [==============================] - 2s 64us/step - loss: 0.0455 - acc: 0.9864 - val_loss: 0.0142 - val_acc: 0.9953\n",
      "\n",
      "Epoch 00003: val_acc improved from 0.99222 to 0.99533, saving model to vgg16_initial_best.h5\n",
      "Epoch 4/10\n",
      "30596/30596 [==============================] - 2s 63us/step - loss: 0.0309 - acc: 0.9916 - val_loss: 0.0110 - val_acc: 0.9959\n",
      "\n",
      "Epoch 00004: val_acc improved from 0.99533 to 0.99591, saving model to vgg16_initial_best.h5\n",
      "Epoch 5/10\n",
      "30596/30596 [==============================] - 2s 63us/step - loss: 0.0267 - acc: 0.9919 - val_loss: 0.0086 - val_acc: 0.9975\n",
      "\n",
      "Epoch 00005: val_acc improved from 0.99591 to 0.99747, saving model to vgg16_initial_best.h5\n",
      "Epoch 6/10\n",
      "30596/30596 [==============================] - 2s 66us/step - loss: 0.0204 - acc: 0.9945 - val_loss: 0.0068 - val_acc: 0.9977\n",
      "\n",
      "Epoch 00006: val_acc improved from 0.99747 to 0.99766, saving model to vgg16_initial_best.h5\n",
      "Epoch 7/10\n",
      "30596/30596 [==============================] - 2s 68us/step - loss: 0.0184 - acc: 0.9946 - val_loss: 0.0059 - val_acc: 0.9982\n",
      "\n",
      "Epoch 00007: val_acc improved from 0.99766 to 0.99825, saving model to vgg16_initial_best.h5\n",
      "Epoch 8/10\n",
      "30596/30596 [==============================] - 2s 68us/step - loss: 0.0146 - acc: 0.9960 - val_loss: 0.0054 - val_acc: 0.9981\n",
      "\n",
      "Epoch 00008: val_acc did not improve from 0.99825\n",
      "Epoch 9/10\n",
      "30596/30596 [==============================] - 2s 64us/step - loss: 0.0144 - acc: 0.9951 - val_loss: 0.0056 - val_acc: 0.9977\n",
      "\n",
      "Epoch 00009: val_acc did not improve from 0.99825\n",
      "Epoch 10/10\n",
      "30596/30596 [==============================] - 2s 63us/step - loss: 0.0111 - acc: 0.9967 - val_loss: 0.0059 - val_acc: 0.9981\n",
      "\n",
      "Epoch 00010: val_acc did not improve from 0.99825\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f2c705dec50>"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "model.fit(x_train_lt5_4d, y_train_lt5,\n",
    "          batch_size = 512,\n",
    "          epochs = 10,\n",
    "          verbose = 1,\n",
    "          callbacks = [checkpoint, early],\n",
    "          validation_data=(x_test_lt5_4d, y_test_lt5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30596/30596 [==============================] - 3s 88us/step\n",
      "5139/5139 [==============================] - 0s 87us/step\n"
     ]
    }
   ],
   "source": [
    "model_score_train = model.evaluate(x_train_lt5_4d, y_train_lt5)\n",
    "model_score_test = model.evaluate(x_test_lt5_4d, y_test_lt5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "my1P09bxAv8H"
   },
   "source": [
    "## 9. Print the training and test accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "yf7F8Gdutbf0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Accuracy: 0.9987906915936724\n",
      "Test accuracy: 0.9980540961276513\n"
     ]
    }
   ],
   "source": [
    "print('Train Accuracy:', model_score_train[1])\n",
    "print('Test accuracy:', model_score_test[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "z78o3WIjaEJ3"
   },
   "source": [
    "## 10. Make only the dense layers to be trainable and convolutional layers to be non-trainable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "brN7VZHFaEJ4"
   },
   "outputs": [],
   "source": [
    "#Freezing layers in the model which don't have 'dense' in their name\n",
    "for layer in model.layers:\n",
    "    if('dense' not in layer.name): #prefix detection to freeze layers which does not have dense\n",
    "    #Freezing a layer\n",
    "        layer.trainable = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34mconv2d_3\u001b[0m\n",
      "\u001b[31mFalse\u001b[0m\n",
      "\u001b[34mactivation_5\u001b[0m\n",
      "\u001b[31mFalse\u001b[0m\n",
      "\u001b[34mconv2d_4\u001b[0m\n",
      "\u001b[31mFalse\u001b[0m\n",
      "\u001b[34mactivation_6\u001b[0m\n",
      "\u001b[31mFalse\u001b[0m\n",
      "\u001b[34mmax_pooling2d_2\u001b[0m\n",
      "\u001b[31mFalse\u001b[0m\n",
      "\u001b[34mdropout_3\u001b[0m\n",
      "\u001b[31mFalse\u001b[0m\n",
      "\u001b[34mflatten_2\u001b[0m\n",
      "\u001b[31mFalse\u001b[0m\n",
      "\u001b[34mdense_3\u001b[0m\n",
      "\u001b[31mTrue\u001b[0m\n",
      "\u001b[34mactivation_7\u001b[0m\n",
      "\u001b[31mFalse\u001b[0m\n",
      "\u001b[34mdropout_4\u001b[0m\n",
      "\u001b[31mFalse\u001b[0m\n",
      "\u001b[34mdense_4\u001b[0m\n",
      "\u001b[31mTrue\u001b[0m\n",
      "\u001b[34mactivation_8\u001b[0m\n",
      "\u001b[31mFalse\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "#Module to print colourful statements\n",
    "from termcolor import colored\n",
    "\n",
    "#Check which layers have been frozen \n",
    "for layer in model.layers:\n",
    "    print (colored(layer.name, 'blue'))\n",
    "    print (colored(layer.trainable, 'red'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4opnW7o0BJ8P"
   },
   "source": [
    "## 11. Use the model trained on 0 to 4 digit classification and train it on the dataset which has digits 5 to 9  (Using Transfer learning keeping only the dense layers to be trainable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "lCFcYHTm6-cE"
   },
   "outputs": [],
   "source": [
    "#The pre-trained weights must exist in a folder called \"data\" in the current folder\n",
    "model.load_weights('./vgg16_initial_best.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "DITyAt3t7Tto"
   },
   "outputs": [],
   "source": [
    "x_train_gte5_4d = np.expand_dims(x_train_gte5, axis = 3)\n",
    "x_test_gte5_4d = np.expand_dims(x_test_gte5, axis = 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New Shape X Train:  (29404, 28, 28, 1)\n",
      "New Shape X Test:  (4861, 28, 28, 1)\n"
     ]
    }
   ],
   "source": [
    "print(\"New Shape X Train: \", x_train_gte5_4d.shape)\n",
    "print(\"New Shape X Test: \", x_test_gte5_4d.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_gte5_4d = x_train_gte5_4d/255\n",
    "x_test_gte5_4d = x_test_gte5_4d/255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_enc = pd.get_dummies(y_train_gte5)\n",
    "y_test_enc = pd.get_dummies(y_test_gte5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_gte5 = y_train_enc\n",
    "y_test_gte5 = y_test_enc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 29404 samples, validate on 4861 samples\n",
      "Epoch 1/10\n",
      "29404/29404 [==============================] - 2s 55us/step - loss: 0.6630 - acc: 0.8150 - val_loss: 0.1196 - val_acc: 0.9617\n",
      "Epoch 2/10\n",
      "29404/29404 [==============================] - 1s 37us/step - loss: 0.1471 - acc: 0.9554 - val_loss: 0.0646 - val_acc: 0.9803\n",
      "Epoch 3/10\n",
      "29404/29404 [==============================] - 1s 34us/step - loss: 0.1060 - acc: 0.9686 - val_loss: 0.0477 - val_acc: 0.9866\n",
      "Epoch 4/10\n",
      "29404/29404 [==============================] - 1s 34us/step - loss: 0.0828 - acc: 0.9753 - val_loss: 0.0410 - val_acc: 0.9879\n",
      "Epoch 5/10\n",
      "29404/29404 [==============================] - 1s 34us/step - loss: 0.0697 - acc: 0.9786 - val_loss: 0.0354 - val_acc: 0.9883\n",
      "Epoch 6/10\n",
      "29404/29404 [==============================] - 1s 34us/step - loss: 0.0654 - acc: 0.9805 - val_loss: 0.0332 - val_acc: 0.9893\n",
      "Epoch 7/10\n",
      "29404/29404 [==============================] - 1s 34us/step - loss: 0.0566 - acc: 0.9820 - val_loss: 0.0326 - val_acc: 0.9901\n",
      "Epoch 8/10\n",
      "29404/29404 [==============================] - 1s 34us/step - loss: 0.0545 - acc: 0.9827 - val_loss: 0.0293 - val_acc: 0.9903\n",
      "Epoch 9/10\n",
      "29404/29404 [==============================] - 1s 34us/step - loss: 0.0484 - acc: 0.9851 - val_loss: 0.0283 - val_acc: 0.9901\n",
      "Epoch 10/10\n",
      "29404/29404 [==============================] - 1s 34us/step - loss: 0.0448 - acc: 0.9860 - val_loss: 0.0262 - val_acc: 0.9907\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f2c7c43f9e8>"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "model.fit(x_train_gte5_4d, y_train_gte5,\n",
    "          batch_size = 512,\n",
    "          epochs = 10,\n",
    "          verbose = 1,\n",
    "          validation_data=(x_test_gte5_4d, y_test_gte5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "SoDozqghCJZ4"
   },
   "source": [
    "## 12. Print the accuracy for classification of digits 5 to 9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "9fCxgb5s49Cj"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "29404/29404 [==============================] - 3s 89us/step\n",
      "4861/4861 [==============================] - 0s 89us/step\n"
     ]
    }
   ],
   "source": [
    "model_score_train = model.evaluate(x_train_gte5_4d, y_train_gte5)\n",
    "model_score_test = model.evaluate(x_test_gte5_4d, y_test_gte5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "LRWizZIpCUKg"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Accuracy: 0.9943885185689022\n",
      "Test accuracy: 0.9907426454358277\n"
     ]
    }
   ],
   "source": [
    "print('Train Accuracy:', model_score_train[1])\n",
    "print('Test accuracy:', model_score_test[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "FU-HwvIdH0M-"
   },
   "source": [
    "## Sentiment analysis <br> \n",
    "\n",
    "The objective of the second problem is to perform Sentiment analysis from the tweets data collected from the users targeted at various mobile devices.\n",
    "Based on the tweet posted by a user (text), we will classify if the sentiment of the user targeted at a particular mobile device is positive or not."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "nAQDiZHRH0M_"
   },
   "source": [
    "### 13. Read the dataset (tweets.csv) and drop the NA's while reading the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 407,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3eXGIe-SH0NA"
   },
   "outputs": [],
   "source": [
    "tweets_data = pd.read_csv(\"tweets.csv\", na_filter = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 408,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "CWeWe1eJH0NF"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 9093 entries, 0 to 9092\n",
      "Data columns (total 3 columns):\n",
      "tweet_text                                            9092 non-null object\n",
      "emotion_in_tweet_is_directed_at                       3291 non-null object\n",
      "is_there_an_emotion_directed_at_a_brand_or_product    9093 non-null object\n",
      "dtypes: object(3)\n",
      "memory usage: 213.2+ KB\n"
     ]
    }
   ],
   "source": [
    "tweets_data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 409,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_text</th>\n",
       "      <th>emotion_in_tweet_is_directed_at</th>\n",
       "      <th>is_there_an_emotion_directed_at_a_brand_or_product</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>.@wesley83 I have a 3G iPhone. After 3 hrs twe...</td>\n",
       "      <td>iPhone</td>\n",
       "      <td>Negative emotion</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>@jessedee Know about @fludapp ? Awesome iPad/i...</td>\n",
       "      <td>iPad or iPhone App</td>\n",
       "      <td>Positive emotion</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>@swonderlin Can not wait for #iPad 2 also. The...</td>\n",
       "      <td>iPad</td>\n",
       "      <td>Positive emotion</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>@sxsw I hope this year's festival isn't as cra...</td>\n",
       "      <td>iPad or iPhone App</td>\n",
       "      <td>Negative emotion</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>@sxtxstate great stuff on Fri #SXSW: Marissa M...</td>\n",
       "      <td>Google</td>\n",
       "      <td>Positive emotion</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          tweet_text  \\\n",
       "0  .@wesley83 I have a 3G iPhone. After 3 hrs twe...   \n",
       "1  @jessedee Know about @fludapp ? Awesome iPad/i...   \n",
       "2  @swonderlin Can not wait for #iPad 2 also. The...   \n",
       "3  @sxsw I hope this year's festival isn't as cra...   \n",
       "4  @sxtxstate great stuff on Fri #SXSW: Marissa M...   \n",
       "\n",
       "  emotion_in_tweet_is_directed_at  \\\n",
       "0                          iPhone   \n",
       "1              iPad or iPhone App   \n",
       "2                            iPad   \n",
       "3              iPad or iPhone App   \n",
       "4                          Google   \n",
       "\n",
       "  is_there_an_emotion_directed_at_a_brand_or_product  \n",
       "0                                   Negative emotion  \n",
       "1                                   Positive emotion  \n",
       "2                                   Positive emotion  \n",
       "3                                   Negative emotion  \n",
       "4                                   Positive emotion  "
      ]
     },
     "execution_count": 409,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "jPJvTjefH0NI"
   },
   "source": [
    "### 14. Preprocess the text and add the preprocessed text in a column with name `text` in the dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 410,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import nltk\n",
    "import string\n",
    "import re\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords \n",
    "stopwords_english = stopwords.words('english')\n",
    " \n",
    "from nltk.stem import PorterStemmer\n",
    "stemmer = PorterStemmer()\n",
    " \n",
    "from nltk.tokenize import TweetTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 411,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "5iec5s9gH0NI"
   },
   "outputs": [],
   "source": [
    "#Referenced from Online Source\n",
    "\n",
    "# Happy Emoticons\n",
    "emoticons_happy = set([\n",
    "    ':-)', ':)', ';)', ':o)', ':]', ':3', ':c)', ':>', '=]', '8)', '=)', ':}',\n",
    "    ':^)', ':-D', ':D', '8-D', '8D', 'x-D', 'xD', 'X-D', 'XD', '=-D', '=D',\n",
    "    '=-3', '=3', ':-))', \":'-)\", \":')\", ':*', ':^*', '>:P', ':-P', ':P', 'X-P',\n",
    "    'x-p', 'xp', 'XP', ':-p', ':p', '=p', ':-b', ':b', '>:)', '>;)', '>:-)',\n",
    "    '<3'\n",
    "    ])\n",
    " \n",
    "# Sad Emoticons\n",
    "emoticons_sad = set([\n",
    "    ':L', ':-/', '>:/', ':S', '>:[', ':@', ':-(', ':[', ':-||', '=L', ':<',\n",
    "    ':-[', ':-<', '=\\\\', '=/', '>:(', ':(', '>.<', \":'-(\", \":'(\", ':\\\\', ':-c',\n",
    "    ':c', ':{', '>:\\\\', ';('\n",
    "    ])\n",
    " \n",
    "# all emoticons (happy + sad)\n",
    "emoticons = emoticons_happy.union(emoticons_sad)\n",
    " \n",
    "def clean_tweets(tweet):\n",
    "\n",
    "    # remove stock market tickers like $GE\n",
    "    tweet = re.sub(r'\\$\\w*', '', tweet)\n",
    " \n",
    "    # remove old style retweet text \"RT\"\n",
    "    tweet = re.sub(r'^RT[\\s]+', '', tweet)\n",
    " \n",
    "    # remove hyperlinks\n",
    "    tweet = re.sub(r'https?:\\/\\/.*[\\r\\n]*', '', tweet)\n",
    "    \n",
    "    # remove hashtags\n",
    "    # only removing the hash # sign from the word\n",
    "    tweet = re.sub(r'#', '', tweet)\n",
    " \n",
    "    # tokenize tweets\n",
    "    tokenizer = TweetTokenizer(preserve_case=False, strip_handles=True, reduce_len=True)\n",
    "    tweet_tokens = tokenizer.tokenize(tweet)\n",
    " \n",
    "    tweets_clean = []    \n",
    "    for word in tweet_tokens:\n",
    "        if (word not in stopwords_english and # remove stopwords\n",
    "              word not in emoticons and # remove emoticons\n",
    "                word not in string.punctuation): # remove punctuation\n",
    "            #tweets_clean.append(word)\n",
    "            stem_word = stemmer.stem(word) # stemming word\n",
    "            tweets_clean.append(stem_word)\n",
    " \n",
    "    return tweets_clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 412,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "EQSmqA-vH0NT"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['hello', 'great', 'day', 'good', 'morn']\n"
     ]
    }
   ],
   "source": [
    "custom_tweet = \"RT @Twitter @chapagain Hello There! Have a great day. :) #good #morning http://chapagain.com.np\"\n",
    " \n",
    "# print cleaned tweet\n",
    "print (clean_tweets(custom_tweet))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 413,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "7kX-WoJDH0NV"
   },
   "outputs": [],
   "source": [
    "tweets_data['text'] = [clean_tweets(str(text)) for text in tweets_data.tweet_text]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 414,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_text</th>\n",
       "      <th>emotion_in_tweet_is_directed_at</th>\n",
       "      <th>is_there_an_emotion_directed_at_a_brand_or_product</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>.@wesley83 I have a 3G iPhone. After 3 hrs twe...</td>\n",
       "      <td>iPhone</td>\n",
       "      <td>Negative emotion</td>\n",
       "      <td>[3g, iphon, 3, hr, tweet, rise_austin, dead, n...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          tweet_text  \\\n",
       "0  .@wesley83 I have a 3G iPhone. After 3 hrs twe...   \n",
       "\n",
       "  emotion_in_tweet_is_directed_at  \\\n",
       "0                          iPhone   \n",
       "\n",
       "  is_there_an_emotion_directed_at_a_brand_or_product  \\\n",
       "0                                   Negative emotion   \n",
       "\n",
       "                                                text  \n",
       "0  [3g, iphon, 3, hr, tweet, rise_austin, dead, n...  "
      ]
     },
     "execution_count": 414,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets_data.head(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "OGWB3P2WH0NY"
   },
   "source": [
    "### 15. Consider only rows having Positive emotion and Negative emotion and remove other rows from the dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 415,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "bdgA_8N2H0NY"
   },
   "outputs": [],
   "source": [
    "valid_emotions = ['Negative emotion', 'Positive emotion']\n",
    "valid_tweets = tweets_data.is_there_an_emotion_directed_at_a_brand_or_product.isin(valid_emotions)\n",
    "invalid_tweets = ~tweets_data.is_there_an_emotion_directed_at_a_brand_or_product.isin(valid_emotions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 416,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_Jlu-reIH0Na"
   },
   "outputs": [],
   "source": [
    "tweets_data_valid = tweets_data[valid_tweets]\n",
    "tweets_data_invalid = tweets_data[invalid_tweets]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 417,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valid Tweets: \n",
      "\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 3548 entries, 0 to 9088\n",
      "Data columns (total 4 columns):\n",
      "tweet_text                                            3548 non-null object\n",
      "emotion_in_tweet_is_directed_at                       3191 non-null object\n",
      "is_there_an_emotion_directed_at_a_brand_or_product    3548 non-null object\n",
      "text                                                  3548 non-null object\n",
      "dtypes: object(4)\n",
      "memory usage: 138.6+ KB\n",
      "None\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_text</th>\n",
       "      <th>emotion_in_tweet_is_directed_at</th>\n",
       "      <th>is_there_an_emotion_directed_at_a_brand_or_product</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>.@wesley83 I have a 3G iPhone. After 3 hrs twe...</td>\n",
       "      <td>iPhone</td>\n",
       "      <td>Negative emotion</td>\n",
       "      <td>[3g, iphon, 3, hr, tweet, rise_austin, dead, n...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          tweet_text  \\\n",
       "0  .@wesley83 I have a 3G iPhone. After 3 hrs twe...   \n",
       "\n",
       "  emotion_in_tweet_is_directed_at  \\\n",
       "0                          iPhone   \n",
       "\n",
       "  is_there_an_emotion_directed_at_a_brand_or_product  \\\n",
       "0                                   Negative emotion   \n",
       "\n",
       "                                                text  \n",
       "0  [3g, iphon, 3, hr, tweet, rise_austin, dead, n...  "
      ]
     },
     "execution_count": 417,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Valid Tweets: \\n\")\n",
    "print(tweets_data_valid.info())\n",
    "tweets_data_valid.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 418,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Invalid Tweets: \n",
      "\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 5545 entries, 5 to 9092\n",
      "Data columns (total 4 columns):\n",
      "tweet_text                                            5544 non-null object\n",
      "emotion_in_tweet_is_directed_at                       100 non-null object\n",
      "is_there_an_emotion_directed_at_a_brand_or_product    5545 non-null object\n",
      "text                                                  5545 non-null object\n",
      "dtypes: object(4)\n",
      "memory usage: 216.6+ KB\n",
      "None\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_text</th>\n",
       "      <th>emotion_in_tweet_is_directed_at</th>\n",
       "      <th>is_there_an_emotion_directed_at_a_brand_or_product</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>@teachntech00 New iPad Apps For #SpeechTherapy...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>No emotion toward brand or product</td>\n",
       "      <td>[new, ipad, app, speechtherapi, commun, showca...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          tweet_text  \\\n",
       "5  @teachntech00 New iPad Apps For #SpeechTherapy...   \n",
       "\n",
       "  emotion_in_tweet_is_directed_at  \\\n",
       "5                             NaN   \n",
       "\n",
       "  is_there_an_emotion_directed_at_a_brand_or_product  \\\n",
       "5                 No emotion toward brand or product   \n",
       "\n",
       "                                                text  \n",
       "5  [new, ipad, app, speechtherapi, commun, showca...  "
      ]
     },
     "execution_count": 418,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Invalid Tweets: \\n\")\n",
    "print(tweets_data_invalid.info())\n",
    "tweets_data_invalid.head(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "SotCRvkDH0Nf"
   },
   "source": [
    "### 16. Represent text as numerical data using `CountVectorizer` and get the document term frequency matrix\n",
    "\n",
    "#### Use `vect` as the variable name for initialising CountVectorizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 456,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "YcbkY4sgH0Ng"
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "vect = CountVectorizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 547,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "KyXtZGr-H0Nl"
   },
   "outputs": [],
   "source": [
    "X_vect = [clean_tweets_for_vect(str(text)) for text in tweets_data.tweet_text]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 548,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "aIdZYxJtH0Nq"
   },
   "outputs": [],
   "source": [
    "doc_term_freq = vect.fit_transform(X_vect)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 549,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "scipy.sparse.csr.csr_matrix"
      ]
     },
     "execution_count": 549,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(doc_term_freq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 550,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'iphon': 3272,\n",
       " 'hr': 3021,\n",
       " 'tweet': 6641,\n",
       " 'rise_austin': 5329,\n",
       " 'dead': 1539,\n",
       " 'need': 4227,\n",
       " 'upgrad': 6767,\n",
       " 'plugin': 4803,\n",
       " 'station': 5980,\n",
       " 'sxsw': 6169,\n",
       " 'know': 3499,\n",
       " 'awesom': 446,\n",
       " 'ipad': 3264,\n",
       " 'app': 275,\n",
       " 'like': 3655,\n",
       " 'appreci': 301,\n",
       " 'design': 1605,\n",
       " 'also': 183,\n",
       " 'they': 6395,\n",
       " 'give': 2605,\n",
       " 'free': 2417,\n",
       " 'ts': 6608,\n",
       " 'wait': 6918,\n",
       " 'sale': 5404,\n",
       " 'hope': 2989,\n",
       " 'year': 7201,\n",
       " 'festiv': 2247,\n",
       " 'crashi': 1402,\n",
       " 'great': 2708,\n",
       " 'stuff': 6061,\n",
       " 'fri': 2428,\n",
       " 'marissa': 3862,\n",
       " 'mayer': 3907,\n",
       " 'googl': 2655,\n",
       " 'tim': 6451,\n",
       " 'reilli': 5199,\n",
       " 'tech': 6293,\n",
       " 'book': 715,\n",
       " 'confer': 1258,\n",
       " 'matt': 3896,\n",
       " 'mullenweg': 4151,\n",
       " 'wordpress': 7123,\n",
       " 'new': 4255,\n",
       " 'speechtherapi': 5900,\n",
       " 'commun': 1215,\n",
       " 'showcas': 5632,\n",
       " 'nan': 4198,\n",
       " 'start': 5970,\n",
       " 'ctia': 1453,\n",
       " 'around': 334,\n",
       " 'corner': 1348,\n",
       " 'googleio': 2665,\n",
       " 'hop': 2988,\n",
       " 'skip': 5706,\n",
       " 'jump': 3413,\n",
       " 'good': 2648,\n",
       " 'time': 6453,\n",
       " 'android': 228,\n",
       " 'fan': 2177,\n",
       " 'beauti': 561,\n",
       " 'smart': 5750,\n",
       " 'simpl': 5663,\n",
       " 'idea': 3076,\n",
       " 'rt': 5375,\n",
       " 'wrote': 7161,\n",
       " 'hollergram': 2960,\n",
       " 'count': 1365,\n",
       " 'day': 1533,\n",
       " 'plu': 4799,\n",
       " 'strong': 6052,\n",
       " 'canadian': 911,\n",
       " 'dollar': 1758,\n",
       " 'mean': 3924,\n",
       " 'stock': 6009,\n",
       " 'appl': 282,\n",
       " 'gear': 2541,\n",
       " 'excit': 2106,\n",
       " 'meet': 3939,\n",
       " 'show': 5631,\n",
       " 'sprint': 5936,\n",
       " 'galaxi': 2502,\n",
       " 'still': 6004,\n",
       " 'run': 5385,\n",
       " 'fail': 2164,\n",
       " 'find': 2278,\n",
       " 'impromptu': 3138,\n",
       " 'parti': 4604,\n",
       " 'foursquar': 2397,\n",
       " 'up': 6762,\n",
       " 'game': 2508,\n",
       " 'gotta': 2681,\n",
       " 'love': 3753,\n",
       " 'calendar': 891,\n",
       " 'featur': 2220,\n",
       " 'top': 6511,\n",
       " 'case': 952,\n",
       " 'check': 1035,\n",
       " 'via': 6853,\n",
       " 'haha': 2784,\n",
       " 'rad': 5068,\n",
       " 'holler': 2959,\n",
       " 'gram': 2696,\n",
       " 'itun': 3310,\n",
       " 'store': 6020,\n",
       " 'notic': 4341,\n",
       " 'dst': 1844,\n",
       " 'come': 1203,\n",
       " 'weekend': 6995,\n",
       " 'mani': 3840,\n",
       " 'user': 6791,\n",
       " 'hour': 3006,\n",
       " 'late': 3548,\n",
       " 'sunday': 6100,\n",
       " 'morn': 4114,\n",
       " 'ad': 61,\n",
       " 'flight': 2325,\n",
       " 'match': 3892,\n",
       " 'peopl': 4674,\n",
       " 'plane': 4774,\n",
       " 'airport': 137,\n",
       " 'download': 1801,\n",
       " 'nice': 4281,\n",
       " 'done': 1765,\n",
       " 'must': 4171,\n",
       " 'review': 5285,\n",
       " 'forb': 2369,\n",
       " 'buy': 870,\n",
       " 'austin': 407,\n",
       " 'sure': 6117,\n",
       " 'll': 3694,\n",
       " 'oh': 4418,\n",
       " 'god': 2629,\n",
       " 'pure': 5008,\n",
       " 'unadulter': 6702,\n",
       " 'easier': 1894,\n",
       " 'brows': 813,\n",
       " 'event': 2068,\n",
       " 'websit': 6988,\n",
       " 'okay': 4423,\n",
       " 'realli': 5123,\n",
       " 'yay': 7194,\n",
       " 'kthxbai': 3510,\n",
       " 'photo': 4721,\n",
       " 'instal': 3214,\n",
       " 'enjoy': 2000,\n",
       " 'chang': 1013,\n",
       " 'gowalla': 2688,\n",
       " 'look': 3728,\n",
       " 'forward': 2391,\n",
       " 'see': 5511,\n",
       " 'els': 1964,\n",
       " 'sleev': 5723,\n",
       " 'smcdalla': 5763,\n",
       " 'pre': 4889,\n",
       " 'wed': 6991,\n",
       " 'win': 7070,\n",
       " 'result': 5265,\n",
       " 'shameless': 5578,\n",
       " 'promot': 4963,\n",
       " 'chevysmc': 1048,\n",
       " 'someon': 5849,\n",
       " 'group': 2730,\n",
       " 'sq': 5938,\n",
       " 'go': 2626,\n",
       " 'rock': 5345,\n",
       " 'updat': 6766,\n",
       " 'push': 5013,\n",
       " 'tonight': 6500,\n",
       " 'right': 5318,\n",
       " 'sweeet': 6140,\n",
       " 'job': 3374,\n",
       " 'team': 6286,\n",
       " 'attn': 387,\n",
       " 'frine': 2439,\n",
       " 'regist': 5189,\n",
       " 'gdgtlive': 2539,\n",
       " 'cobra': 1179,\n",
       " 'iradar': 3280,\n",
       " 'link': 3666,\n",
       " 'anyon': 257,\n",
       " 'want': 6939,\n",
       " 'sell': 5525,\n",
       " 'old': 4425,\n",
       " 'bought': 743,\n",
       " 'older': 4426,\n",
       " 'oooh': 4457,\n",
       " 'launch': 3562,\n",
       " 'major': 3824,\n",
       " 'social': 5811,\n",
       " 'network': 4249,\n",
       " 'call': 895,\n",
       " 'circl': 1104,\n",
       " 'possibl': 4857,\n",
       " 'today': 6482,\n",
       " 'best': 603,\n",
       " 'ha': 2773,\n",
       " 'first': 2291,\n",
       " 'line': 3664,\n",
       " 'pop': 4831,\n",
       " 'planner': 4776,\n",
       " 'eventprof': 2071,\n",
       " 'pcma': 4645,\n",
       " 'engag': 1995,\n",
       " 'spin': 5913,\n",
       " 'play': 4782,\n",
       " 'concept': 1245,\n",
       " 'music': 4161,\n",
       " 'discoveri': 1693,\n",
       " 'com': 1200,\n",
       " 'fals': 2171,\n",
       " 'alarm': 150,\n",
       " 'now창': 4360,\n",
       " 'probabl': 4940,\n",
       " 'ever': 2075,\n",
       " 'vatornew': 6821,\n",
       " 'forc': 2371,\n",
       " 'print': 4928,\n",
       " 'media': 3932,\n",
       " 'evolv': 2090,\n",
       " 'weather': 6973,\n",
       " 'greet': 2715,\n",
       " 'sweater': 6139,\n",
       " 'night': 4290,\n",
       " 'put': 5018,\n",
       " 'flash': 2307,\n",
       " 'downtown': 1806,\n",
       " 'hootsuit': 2987,\n",
       " 'mobil': 4066,\n",
       " 'blackberri': 660,\n",
       " 'whether': 7023,\n",
       " 'you창': 7228,\n",
       " 'get': 2578,\n",
       " 'friend': 2434,\n",
       " 'hey': 2900,\n",
       " 'long': 3722,\n",
       " 'think': 6402,\n",
       " 'take': 6243,\n",
       " 'us': 6781,\n",
       " 'make': 3825,\n",
       " 'answer': 244,\n",
       " 'use': 6788,\n",
       " 'zazzlesxsw': 7245,\n",
       " 'we창': 7012,\n",
       " 'one': 4442,\n",
       " 'mashabl': 3880,\n",
       " 'video': 6860,\n",
       " 'gadget': 2493,\n",
       " 'pad': 4553,\n",
       " 'ubersoci': 6676,\n",
       " 'includ': 3156,\n",
       " 'uberguid': 6674,\n",
       " 'sponsor': 5923,\n",
       " 'smartcover창': 5751,\n",
       " 'open': 4462,\n",
       " 'instant': 3215,\n",
       " 'access': 35,\n",
       " 'hand': 2799,\n",
       " 'held': 2887,\n",
       " 'hobo창': 2951,\n",
       " 'drafthous': 1810,\n",
       " 'hobo': 2950,\n",
       " 'shotgun창': 5627,\n",
       " 'hooray': 2984,\n",
       " 'orli': 4492,\n",
       " 'set': 5554,\n",
       " 'sxsw창': 6215,\n",
       " 'wooo': 7118,\n",
       " 'til': 6449,\n",
       " 'midnight': 4000,\n",
       " 'khoi': 3453,\n",
       " 'vinh': 6866,\n",
       " 'say': 5438,\n",
       " 'cond': 1252,\n",
       " 'nast': 4204,\n",
       " 'headlong': 2858,\n",
       " 'rush': 5387,\n",
       " 'publish': 4995,\n",
       " 'fundament': 2470,\n",
       " 'misunderstand': 4048,\n",
       " 'platform': 4781,\n",
       " 'help': 2890,\n",
       " 'doc': 1741,\n",
       " 'anonym': 241,\n",
       " 'account': 44,\n",
       " 'techi': 6300,\n",
       " 'ppl': 4880,\n",
       " 'jam': 3330,\n",
       " 'libya': 3634,\n",
       " 'edchat': 1916,\n",
       " 'musedchat': 4159,\n",
       " 'sxswi': 6188,\n",
       " 'classic': 1122,\n",
       " 'newtwitt': 4269,\n",
       " 'location': 3705,\n",
       " 'bas': 519,\n",
       " 'fast': 2197,\n",
       " 'fun': 2468,\n",
       " 'futur': 2479,\n",
       " 'connect': 1270,\n",
       " 'digit': 1659,\n",
       " 'physic': 4728,\n",
       " 'world': 7133,\n",
       " 'talk': 6253,\n",
       " 'google': 2656,\n",
       " 'effort': 1936,\n",
       " 'allow': 169,\n",
       " 'system': 6231,\n",
       " 'bettercloud': 611,\n",
       " 'speak': 5888,\n",
       " 'mark': 3866,\n",
       " 'belinski': 585,\n",
       " 'panel': 4576,\n",
       " 'st': 5950,\n",
       " 'stop': 6017,\n",
       " 'chao': 1016,\n",
       " 'hunt': 3043,\n",
       " 'java': 3342,\n",
       " 'spi': 5908,\n",
       " 'chanc': 1011,\n",
       " 'pic': 4731,\n",
       " 'guy': 2768,\n",
       " 'kawasaki': 3434,\n",
       " 'enchant': 1987,\n",
       " 'internet': 3235,\n",
       " 'futuremf': 2482,\n",
       " 'spec': 5893,\n",
       " 'recip': 5145,\n",
       " 'web': 6974,\n",
       " 'search': 5498,\n",
       " 'omfg': 4435,\n",
       " 'heard': 2867,\n",
       " 'apple': 285,\n",
       " 'alreadi': 180,\n",
       " 'smile': 5768,\n",
       " 'would': 7145,\n",
       " 'lot': 3744,\n",
       " 'interest': 3230,\n",
       " 'actual': 60,\n",
       " 'insan': 3203,\n",
       " 'agre': 117,\n",
       " 'fiona': 2285,\n",
       " 'town': 6535,\n",
       " 'somebodi': 5846,\n",
       " 'kidnap': 3461,\n",
       " 'record': 5157,\n",
       " 'studio': 6060,\n",
       " 'album': 152,\n",
       " 'wanna': 6936,\n",
       " 'drink': 1821,\n",
       " 'pm': 4810,\n",
       " 'fado': 2163,\n",
       " 'th': 6352,\n",
       " 'join': 3383,\n",
       " 'attend': 381,\n",
       " 'headach': 2856,\n",
       " 'booo': 723,\n",
       " 'flipboard': 2327,\n",
       " 'develop': 1626,\n",
       " 'version': 6846,\n",
       " 'power': 4871,\n",
       " 'valu': 6812,\n",
       " 'locat': 3704,\n",
       " 'base': 520,\n",
       " 'servic': 5547,\n",
       " 'battl': 540,\n",
       " 'in': 3145,\n",
       " 'fatigu': 2206,\n",
       " 'pnid': 4811,\n",
       " 'chilcott': 1055,\n",
       " 'stand': 5962,\n",
       " 'blogger': 681,\n",
       " 'staff': 5954,\n",
       " 'competit': 1229,\n",
       " 'mention': 3963,\n",
       " 'shirt': 5610,\n",
       " 'band': 495,\n",
       " 'food': 2358,\n",
       " 'art': 342,\n",
       " 'ice': 3067,\n",
       " 'cream': 1409,\n",
       " 'nifti': 4288,\n",
       " 'interact': 3228,\n",
       " 'map': 3846,\n",
       " 'promis': 4961,\n",
       " 'groupon': 2733,\n",
       " 'rewards': 5290,\n",
       " 'typ': 6667,\n",
       " 'finger': 2281,\n",
       " 'cross': 1434,\n",
       " 'news': 4262,\n",
       " 'yahoo': 7189,\n",
       " 'loos': 3735,\n",
       " 'traffic': 6549,\n",
       " 'site': 5683,\n",
       " 'doubt': 1792,\n",
       " 'last': 3546,\n",
       " 'tho': 6410,\n",
       " 'weird': 7000,\n",
       " 'name': 4197,\n",
       " 'holla': 2958,\n",
       " 'butt': 868,\n",
       " 'can': 909,\n",
       " 'phone': 4719,\n",
       " 'worship': 7139,\n",
       " 'droid': 1827,\n",
       " 'mac': 3794,\n",
       " 'agnerd': 115,\n",
       " 'confess': 1259,\n",
       " 'laptop': 3539,\n",
       " 'follow': 2353,\n",
       " 'deni': 1590,\n",
       " 'debut': 1549,\n",
       " 'despit': 1614,\n",
       " 'report': 5236,\n",
       " 'post': 4858,\n",
       " 'easi': 1893,\n",
       " 'behav': 577,\n",
       " 'crash': 1400,\n",
       " 'yesterday': 7209,\n",
       " 'ridicul': 5312,\n",
       " 'popup': 4838,\n",
       " 'peek': 4660,\n",
       " 'space': 5878,\n",
       " 'that': 6361,\n",
       " 'slate': 5718,\n",
       " 'tomorrow': 6496,\n",
       " 'thing': 6400,\n",
       " 'earth': 1889,\n",
       " 'face': 2152,\n",
       " 'compani': 1221,\n",
       " 'sxwsi': 6218,\n",
       " 'stay': 5984,\n",
       " 'tune': 6627,\n",
       " 'hcker': 2848,\n",
       " 'thank': 6354,\n",
       " 'touchingstori': 6526,\n",
       " 'preso': 4904,\n",
       " 'here': 2892,\n",
       " 'deck': 1556,\n",
       " 'tri': 6575,\n",
       " 'contact': 1296,\n",
       " 'famili': 2174,\n",
       " 'japan': 3336,\n",
       " 'creat': 1410,\n",
       " 'person': 4699,\n",
       " 'finder': 2279,\n",
       " 'speech': 5899,\n",
       " 'conf': 1257,\n",
       " 'sxswh': 6186,\n",
       " 'sxsh': 6164,\n",
       " 'medic': 3935,\n",
       " 'blog': 679,\n",
       " 'mhealth': 3986,\n",
       " 'provid': 4980,\n",
       " 'charger': 1019,\n",
       " 'mind': 4018,\n",
       " 'next': 4271,\n",
       " 'wonder': 7114,\n",
       " 'flashmob': 2308,\n",
       " 'tcrn': 6279,\n",
       " 'ch': 1003,\n",
       " 'fcsj': 2217,\n",
       " 'tip': 6463,\n",
       " 'api': 273,\n",
       " 'suxsw': 6132,\n",
       " 'xma': 7180,\n",
       " 'shini': 5606,\n",
       " 'christma': 1086,\n",
       " 'nerd': 4232,\n",
       " 'yai': 7190,\n",
       " 'cont': 1295,\n",
       " 'ye': 7196,\n",
       " 'got': 2680,\n",
       " 'anoth': 242,\n",
       " 'gem': 2560,\n",
       " 'present': 4903,\n",
       " 'local': 3702,\n",
       " 'gsd': 2742,\n",
       " 'industri': 3175,\n",
       " 'weliveher': 7002,\n",
       " 'gsdm': 2743,\n",
       " 'buzz': 872,\n",
       " 'headlin': 2857,\n",
       " 'hav': 2835,\n",
       " 'hmm': 2943,\n",
       " 'could': 1363,\n",
       " 'seen': 5515,\n",
       " 'wow': 7146,\n",
       " 'dataviz': 1526,\n",
       " 'translat': 6560,\n",
       " 'satan': 5427,\n",
       " 'sayin': 5440,\n",
       " 'checkin': 1036,\n",
       " 'month': 4101,\n",
       " 'ago': 116,\n",
       " 'ok': 4422,\n",
       " 'out': 4502,\n",
       " 'bizzi': 656,\n",
       " 'left': 3600,\n",
       " 'brain': 761,\n",
       " 'bettersearch': 612,\n",
       " 'engin': 1996,\n",
       " 'hp': 3018,\n",
       " 'park': 4598,\n",
       " 'content': 1299,\n",
       " 'incub': 3162,\n",
       " 'construct': 1288,\n",
       " 'lewi': 3630,\n",
       " 'level': 3628,\n",
       " 'reason': 5130,\n",
       " 'continu': 1306,\n",
       " 'exist': 2115,\n",
       " 'evid': 2087,\n",
       " 'bawl': 545,\n",
       " 'pagemak': 4557,\n",
       " 'save': 5431,\n",
       " 'jwtatl': 3426,\n",
       " 'spark': 5884,\n",
       " 'teamandroid': 6288,\n",
       " 'award': 443,\n",
       " 'read': 5112,\n",
       " 'unbox': 6705,\n",
       " 'thought': 6415,\n",
       " 'apac': 268,\n",
       " 'region': 5188,\n",
       " 'deal': 1543,\n",
       " 'earthquak': 1891,\n",
       " 'tsunami': 6612,\n",
       " 'trauma': 6565,\n",
       " 'school': 5468,\n",
       " 'market': 3868,\n",
       " 'expert': 2127,\n",
       " 'cnet': 1167,\n",
       " 'temporari': 6325,\n",
       " 'def': 1562,\n",
       " 'tent': 6333,\n",
       " 'powerhous': 4873,\n",
       " 'gym': 2772,\n",
       " 'congress': 1269,\n",
       " 'along': 175,\n",
       " 'happi': 2812,\n",
       " 'hipster': 2925,\n",
       " 'support': 6113,\n",
       " 'trend': 6574,\n",
       " 'nerdi': 4236,\n",
       " 'head': 2855,\n",
       " 'newbi': 4258,\n",
       " 'funni': 2473,\n",
       " 'matter': 3897,\n",
       " 'minut': 4035,\n",
       " 'point': 4818,\n",
       " 'least': 3595,\n",
       " 'accord': 42,\n",
       " 'twitter': 6658,\n",
       " 'christian': 1085,\n",
       " 'dev': 1624,\n",
       " 'mayb': 3906,\n",
       " 'wk': 7098,\n",
       " 'togeth': 6486,\n",
       " 'cool': 1331,\n",
       " 'ux': 6799,\n",
       " 'uxd': 6801,\n",
       " 'rememb': 5219,\n",
       " 'ultim': 6691,\n",
       " 'awar': 442,\n",
       " 'audienc': 396,\n",
       " 'target': 6266,\n",
       " 'toward': 6532,\n",
       " 'unexpect': 6726,\n",
       " 'experi': 2124,\n",
       " 'taken': 6245,\n",
       " 'storm': 6022,\n",
       " 'part': 4601,\n",
       " 'haz': 2844,\n",
       " 'ifrom': 3092,\n",
       " 'gr': 2693,\n",
       " 'stack': 5953,\n",
       " 'mine': 4024,\n",
       " 'hassl': 2831,\n",
       " 'handl': 2802,\n",
       " 'perfectli': 4687,\n",
       " 'elus': 1967,\n",
       " 'eurorscg': 2057,\n",
       " 'notatsxsw': 4328,\n",
       " 'nearbi': 4221,\n",
       " 'pep': 4678,\n",
       " 'smallbiz': 5745,\n",
       " 'place': 4768,\n",
       " 'seo': 5535,\n",
       " 'samsung': 5414,\n",
       " 'soni': 5855,\n",
       " 'lead': 3583,\n",
       " 'atx': 391,\n",
       " 'rg': 5297,\n",
       " 'south': 5873,\n",
       " 'korean': 3507,\n",
       " 'director': 1678,\n",
       " 'movi': 4132,\n",
       " 'entir': 2011,\n",
       " 'prep': 4899,\n",
       " 'amaz': 194,\n",
       " 'focu': 2347,\n",
       " 'own': 4541,\n",
       " 'billboard': 635,\n",
       " 'prchat': 4888,\n",
       " 'share': 5581,\n",
       " 'gather': 2526,\n",
       " 'info': 3187,\n",
       " 'turn': 6631,\n",
       " 'busi': 862,\n",
       " 'card': 931,\n",
       " 'broadcast': 802,\n",
       " 'btw': 825,\n",
       " 'sold': 5838,\n",
       " 'model': 4077,\n",
       " 'vzw': 6910,\n",
       " 'sneaki': 5799,\n",
       " 'usual': 6794,\n",
       " 'beta': 608,\n",
       " 'test': 6343,\n",
       " 'moonbot': 4105,\n",
       " 'louisiana': 3750,\n",
       " 'ton': 6497,\n",
       " 'fastbal': 2198,\n",
       " 'away': 444,\n",
       " 'two': 6662,\n",
       " 'wifi': 7056,\n",
       " 'black': 658,\n",
       " 'cover': 1376,\n",
       " 'fo': 2345,\n",
       " 'attsxsw': 389,\n",
       " 'bo': 700,\n",
       " 'lt': 3768,\n",
       " 'hous': 3007,\n",
       " 'worth': 7141,\n",
       " 'trek': 6571,\n",
       " 'convent': 1318,\n",
       " 'center': 989,\n",
       " 'everyth': 2083,\n",
       " 'except': 2104,\n",
       " 'gig': 2593,\n",
       " 'white창': 7033,\n",
       " 'manag': 3837,\n",
       " 'known': 3501,\n",
       " 'white': 7030,\n",
       " 'jean': 3352,\n",
       " 'configur': 1260,\n",
       " 'offer': 4408,\n",
       " 'promo': 4962,\n",
       " 'ninjafind': 4302,\n",
       " 'suck': 6082,\n",
       " 'film': 2265,\n",
       " 'shipment': 5609,\n",
       " 'yet': 7210,\n",
       " 'marc': 3853,\n",
       " 'ecko': 1910,\n",
       " 'autodi': 426,\n",
       " 'polit': 4823,\n",
       " 'edreform': 1923,\n",
       " 'edtech': 1924,\n",
       " 'eduvc': 1928,\n",
       " 'fightthepaddl': 2261,\n",
       " 'poursit': 4869,\n",
       " 'learn': 3591,\n",
       " 'life': 3639,\n",
       " 'impact': 3126,\n",
       " 'real': 5118,\n",
       " 'people': 4675,\n",
       " 'live': 3682,\n",
       " 'bravo': 768,\n",
       " 'lonelyplanet': 3721,\n",
       " 'guid': 2756,\n",
       " 'limit': 3659,\n",
       " 'lp': 3767,\n",
       " 'travel': 6566,\n",
       " 'mp': 4134,\n",
       " 'aclu': 50,\n",
       " 'owt': 4543,\n",
       " 'prompt': 4964,\n",
       " 'memori': 3959,\n",
       " 'describ': 1602,\n",
       " 'childhood': 1057,\n",
       " 'walk': 6925,\n",
       " 'short': 5619,\n",
       " 'essenti': 2044,\n",
       " 'tool': 6506,\n",
       " 'tradeshow': 6547,\n",
       " 'demo': 1584,\n",
       " 'theatr': 6368,\n",
       " 'realtim': 5128,\n",
       " 'four': 2396,\n",
       " 'listen': 3678,\n",
       " 'realiz': 5122,\n",
       " 'spoken': 5921,\n",
       " 'pick': 4733,\n",
       " 'will': 7064,\n",
       " 'pay': 4635,\n",
       " 'grab': 2694,\n",
       " 'quick': 5052,\n",
       " 'hundr': 3039,\n",
       " 'hoc': 2952,\n",
       " 'cost': 1357,\n",
       " 'solv': 5845,\n",
       " 'induc': 3174,\n",
       " 'iphone': 3273,\n",
       " 'toilet': 6487,\n",
       " 'crisi': 1427,\n",
       " 'monday': 4090,\n",
       " 'barri': 512,\n",
       " 'diller': 1666,\n",
       " 'york': 7216,\n",
       " 'lunch': 3778,\n",
       " 'hotel': 3000,\n",
       " 'six': 5687,\n",
       " 'dirti': 1680,\n",
       " 'martini': 3876,\n",
       " 'serious': 5542,\n",
       " 'constant': 1285,\n",
       " 'caus': 969,\n",
       " 'lost': 3743,\n",
       " 'schedul': 5460,\n",
       " 'sync': 6226,\n",
       " 'wp': 7149,\n",
       " 'readi': 5116,\n",
       " 'ur': 6776,\n",
       " 'conflagr': 1263,\n",
       " 'doofus': 1778,\n",
       " 'attent': 384,\n",
       " 'er': 2032,\n",
       " 'rumor': 5383,\n",
       " 'went': 7005,\n",
       " 'lousi': 3752,\n",
       " 'folk': 2352,\n",
       " 'secur': 5509,\n",
       " 'bsidesaustin': 823,\n",
       " 'work': 7125,\n",
       " 'hire': 2928,\n",
       " 'io': 3261,\n",
       " 'man': 3836,\n",
       " 'let': 3622,\n",
       " 'pictur': 4738,\n",
       " 'what': 7014,\n",
       " 'hit': 2935,\n",
       " 'spent': 5907,\n",
       " 'coupl': 1369,\n",
       " 'citi': 1113,\n",
       " 'block': 678,\n",
       " 'behind': 580,\n",
       " 'email': 1969,\n",
       " 'compos': 1238,\n",
       " 'repli': 5234,\n",
       " 'protip': 4973,\n",
       " 'song': 5854,\n",
       " 'alon': 174,\n",
       " 'million': 4015,\n",
       " 'current': 1471,\n",
       " 'avail': 432,\n",
       " 'gb': 2533,\n",
       " 'onli': 4445,\n",
       " 'option': 4475,\n",
       " 'leather': 3596,\n",
       " 'entertain': 2008,\n",
       " '_pleas': 4,\n",
       " 'don창': 1775,\n",
       " 'grate': 2704,\n",
       " 'less': 3620,\n",
       " 'announc': 238,\n",
       " 'detail': 1619,\n",
       " 'giveaway': 2606,\n",
       " 'wild': 7061,\n",
       " 'terribl': 6339,\n",
       " 'queue': 5049,\n",
       " 'cours': 1371,\n",
       " 'snap': 5792,\n",
       " 'keynot': 3449,\n",
       " 'slide': 5727,\n",
       " 'credit': 1416,\n",
       " 'click': 1135,\n",
       " 'purchas': 5007,\n",
       " 'nd': 4218,\n",
       " 'cocktail': 1182,\n",
       " 'texa': 6346,\n",
       " 'snowflak': 5803,\n",
       " 'cnngrill': 1169,\n",
       " 'we': 6967,\n",
       " 'southwest': 5877,\n",
       " 'sweet': 6143,\n",
       " 'ballroom': 490,\n",
       " 'mile': 4010,\n",
       " 'per': 4682,\n",
       " 'drive': 1824,\n",
       " 'navig': 4211,\n",
       " 'longer': 3723,\n",
       " 'wider': 7050,\n",
       " 'pack': 4549,\n",
       " 'extra': 2142,\n",
       " 'way': 6962,\n",
       " 'sunni': 6104,\n",
       " 'stow': 6024,\n",
       " 'sqwill': 5946,\n",
       " 'attempt': 380,\n",
       " 'facebook': 2153,\n",
       " 'lbswar': 3580,\n",
       " 'there': 6387,\n",
       " 'grow': 2734,\n",
       " 'step': 5994,\n",
       " 'bulletproof': 846,\n",
       " 'strategi': 6032,\n",
       " 'rhjr_ux': 5301,\n",
       " 'wander': 6934,\n",
       " 'street': 6038,\n",
       " 'girl': 2599,\n",
       " 'cevich': 1002,\n",
       " 'mojito': 4079,\n",
       " 'juic': 3406,\n",
       " 'track': 6541,\n",
       " 'stage': 5955,\n",
       " 'frostwir': 2446,\n",
       " 'wi': 7047,\n",
       " 'fi': 2256,\n",
       " 'week': 6994,\n",
       " 'ahead': 121,\n",
       " 'amazon': 196,\n",
       " 'believ': 584,\n",
       " 'aussi': 406,\n",
       " 'buyer': 871,\n",
       " 'spreadsheet': 5931,\n",
       " 'o_o': 4387,\n",
       " 'plan': 4772,\n",
       " 'sixth': 5688,\n",
       " 'told': 6491,\n",
       " 'car': 928,\n",
       " 'zimrid': 7256,\n",
       " 'etc': 2050,\n",
       " 'ride': 5310,\n",
       " 'shareabl': 5582,\n",
       " 'tablet': 6235,\n",
       " 'everi': 2077,\n",
       " 'singl': 5674,\n",
       " 'receiv': 5141,\n",
       " 'messag': 3974,\n",
       " 'tell': 6322,\n",
       " 'sexi': 5562,\n",
       " 'respons': 5257,\n",
       " 'fellow': 2237,\n",
       " 'mophi': 4110,\n",
       " 'batteri': 538,\n",
       " 'lug': 3774,\n",
       " 'huge': 3031,\n",
       " 'took': 6505,\n",
       " 'piss': 4755,\n",
       " 'trade': 6546,\n",
       " 'kiiill': 3463,\n",
       " 'monger': 4094,\n",
       " '창_': 7272,\n",
       " 'preview': 4914,\n",
       " 'socbiz': 5808,\n",
       " 'fb': 2213,\n",
       " 'shotgun': 5626,\n",
       " 'pleas': 4793,\n",
       " 'build': 842,\n",
       " 'biggest': 627,\n",
       " 'company': 1222,\n",
       " 'histori': 2933,\n",
       " 'survey': 6127,\n",
       " 'starbuck': 5967,\n",
       " 'gift': 2592,\n",
       " 'research': 5249,\n",
       " 'gratif': 2705,\n",
       " 'retail': 5268,\n",
       " 'releas': 5207,\n",
       " 'groupm': 2732,\n",
       " 'sound': 5868,\n",
       " 'incred': 3161,\n",
       " 'reveal': 5281,\n",
       " 'closur': 1146,\n",
       " 'made': 3806,\n",
       " 'question': 5048,\n",
       " 'insid': 3206,\n",
       " 'fire': 2286,\n",
       " 'paraorm': 4593,\n",
       " 'activ': 57,\n",
       " 'sanctuari': 5418,\n",
       " 'sketch': 5693,\n",
       " 'ar': 314,\n",
       " 'magic': 3816,\n",
       " 'lens': 3615,\n",
       " 'eduar': 1926,\n",
       " 'iie': 3101,\n",
       " 'si': 5643,\n",
       " 'swimsuit': 6147,\n",
       " 'issu': 3296,\n",
       " 'garag': 2517,\n",
       " 'add': 64,\n",
       " 'instagram': 3213,\n",
       " 'socialsearch': 5824,\n",
       " 'aftrnoon': 106,\n",
       " 'frozen': 2448,\n",
       " 'lake': 3521,\n",
       " 'photograph': 4723,\n",
       " 'brother': 809,\n",
       " 'scott': 5480,\n",
       " 'catch': 961,\n",
       " 'weeknd': 6996,\n",
       " 'botch': 739,\n",
       " 'timechang': 6454,\n",
       " 'freak': 2416,\n",
       " 'miss': 4040,\n",
       " 'bloodi': 686,\n",
       " 'mari': 3860,\n",
       " 'shout': 5630,\n",
       " 'ladi': 3519,\n",
       " 'hold': 2955,\n",
       " 'sip': 5677,\n",
       " 'beer': 569,\n",
       " 'cc': 976,\n",
       " 'dj': 1731,\n",
       " 'hoodi': 2979,\n",
       " 'allen': 166,\n",
       " 'due': 1857,\n",
       " 'respect': 5256,\n",
       " 'back': 461,\n",
       " 'meant': 3926,\n",
       " 'wish': 7089,\n",
       " 'dyac': 1877,\n",
       " 'stupid': 6066,\n",
       " 'america': 201,\n",
       " 'audioboo': 398,\n",
       " 'american': 202,\n",
       " 'dream': 1817,\n",
       " 'bust': 864,\n",
       " 'bum': 849,\n",
       " 'begger': 573,\n",
       " 'socialart': 5814,\n",
       " 'document': 1745,\n",
       " 'skifta': 5702,\n",
       " 'shift': 5602,\n",
       " 'sign': 5651,\n",
       " 'lock': 3707,\n",
       " 'curat': 1465,\n",
       " 'brilliant': 795,\n",
       " 'enlighten': 2001,\n",
       " 'mechan': 3931,\n",
       " 'sit': 5681,\n",
       " 'tree': 6570,\n",
       " 'unsix': 6754,\n",
       " 'thirsti': 6406,\n",
       " 'chevi': 1047,\n",
       " 'volt': 6895,\n",
       " 'loung': 3751,\n",
       " 'other': 4498,\n",
       " 'came': 902,\n",
       " 'chosen': 1082,\n",
       " 'random': 5093,\n",
       " 'skream': 5708,\n",
       " 'wall': 6927,\n",
       " 'appropri': 303,\n",
       " 'geniu': 2565,\n",
       " 'arduino': 323,\n",
       " 'flame': 2303,\n",
       " 'skull': 5710,\n",
       " 'refriger': 5182,\n",
       " 'salon': 5408,\n",
       " 'smartth': 5758,\n",
       " 'remind': 5220,\n",
       " 'chrome': 1087,\n",
       " 'tt': 6614,\n",
       " 'vs': 6905,\n",
       " 'backupifi': 468,\n",
       " 'cloudsight': 1153,\n",
       " 'searchabl': 5499,\n",
       " 'archiv': 320,\n",
       " 'solut': 5844,\n",
       " 'myopia': 4183,\n",
       " 'joke': 3385,\n",
       " 'dictaphon': 1643,\n",
       " 'vid': 6859,\n",
       " 'camera': 903,\n",
       " 'cerebellum': 996,\n",
       " 'charg': 1018,\n",
       " 'anyth': 258,\n",
       " 'prop': 4966,\n",
       " 'found': 2394,\n",
       " 'kype': 3514,\n",
       " 'geoloc': 2571,\n",
       " 'background': 462,\n",
       " 'patch': 4619,\n",
       " 'batterykil': 539,\n",
       " 'rsvp': 5374,\n",
       " 'sundayswagger': 6102,\n",
       " 'eventbrite': 2070,\n",
       " 'scoremor': 5479,\n",
       " 'screen': 5485,\n",
       " 'nut': 4377,\n",
       " 'break': 775,\n",
       " 'partnership': 4608,\n",
       " 'porn': 4844,\n",
       " 'chat': 1028,\n",
       " 'facetim': 2155,\n",
       " 'built': 843,\n",
       " ...}"
      ]
     },
     "execution_count": 550,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vect.vocabulary_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5pxd5fSHH0Nt"
   },
   "source": [
    "### 17. Find number of different words in vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 423,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "p1DQ2LdNH0Nu"
   },
   "outputs": [],
   "source": [
    "def extract_features(tweet):\n",
    "    tweet_words = set(tweet)\n",
    "    features = {}\n",
    "    for word in word_features:\n",
    "        features['contains(%s)' % word] = (word in tweet_words)\n",
    "    return features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "dwtgjTBeH0Ny"
   },
   "source": [
    "#### Tip: To see all available functions for an Object use dir"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ShA6D8jKH0N5"
   },
   "source": [
    "### 18. Find out how many Positive and Negative emotions are there.\n",
    "\n",
    "Hint: Use value_counts on that column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 424,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "q7LAl5pzH0N6"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Positive emotion    2978\n",
       "Negative emotion     570\n",
       "Name: is_there_an_emotion_directed_at_a_brand_or_product, dtype: int64"
      ]
     },
     "execution_count": 424,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets_data_valid.is_there_an_emotion_directed_at_a_brand_or_product.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "IUvgj0FoH0N9"
   },
   "source": [
    "### 19. Change the labels for Positive and Negative emotions as 1 and 0 respectively and store in a different column in the same dataframe named 'Label'\n",
    "\n",
    "Hint: use map on that column and give labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 425,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "YftKwFv7H0N9"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_text</th>\n",
       "      <th>emotion_in_tweet_is_directed_at</th>\n",
       "      <th>is_there_an_emotion_directed_at_a_brand_or_product</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>.@wesley83 I have a 3G iPhone. After 3 hrs twe...</td>\n",
       "      <td>iPhone</td>\n",
       "      <td>0</td>\n",
       "      <td>[3g, iphon, 3, hr, tweet, rise_austin, dead, n...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>@jessedee Know about @fludapp ? Awesome iPad/i...</td>\n",
       "      <td>iPad or iPhone App</td>\n",
       "      <td>1</td>\n",
       "      <td>[know, awesom, ipad, iphon, app, like, appreci...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>@swonderlin Can not wait for #iPad 2 also. The...</td>\n",
       "      <td>iPad</td>\n",
       "      <td>1</td>\n",
       "      <td>[wait, ipad, 2, also, sale, sxsw]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          tweet_text  \\\n",
       "0  .@wesley83 I have a 3G iPhone. After 3 hrs twe...   \n",
       "1  @jessedee Know about @fludapp ? Awesome iPad/i...   \n",
       "2  @swonderlin Can not wait for #iPad 2 also. The...   \n",
       "\n",
       "  emotion_in_tweet_is_directed_at  \\\n",
       "0                          iPhone   \n",
       "1              iPad or iPhone App   \n",
       "2                            iPad   \n",
       "\n",
       "   is_there_an_emotion_directed_at_a_brand_or_product  \\\n",
       "0                                                  0    \n",
       "1                                                  1    \n",
       "2                                                  1    \n",
       "\n",
       "                                                text  \n",
       "0  [3g, iphon, 3, hr, tweet, rise_austin, dead, n...  \n",
       "1  [know, awesom, ipad, iphon, app, like, appreci...  \n",
       "2                  [wait, ipad, 2, also, sale, sxsw]  "
      ]
     },
     "execution_count": 425,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "binary_nums = {\"is_there_an_emotion_directed_at_a_brand_or_product\": {\"Negative emotion\": 0, \"Positive emotion\": 1}}\n",
    "tweets_data_valid = tweets_data_valid.replace(binary_nums)\n",
    "tweets_data_valid.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "3YErwYLCH0N_"
   },
   "source": [
    "### 20. Define the feature set (independent variable or X) to be `text` column and `labels` as target (or dependent variable)  and divide into train and test datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 389,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "lNkwrGgEH0OA"
   },
   "outputs": [],
   "source": [
    "X = tweets_data_valid.text\n",
    "Y = tweets_data_valid.is_there_an_emotion_directed_at_a_brand_or_product\n",
    "tweets_final_data = tweets_data_valid.iloc[:, [2,3]]\n",
    "word_features = buildVocabulary(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 428,
   "metadata": {},
   "outputs": [],
   "source": [
    "neg = tweets_data_valid[tweets_data_valid.is_there_an_emotion_directed_at_a_brand_or_product == 0]\n",
    "pos = tweets_data_valid[tweets_data_valid.is_there_an_emotion_directed_at_a_brand_or_product == 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 429,
   "metadata": {},
   "outputs": [],
   "source": [
    "#starting the function \n",
    "def featureExtraction(tweets_and_sentiments):\n",
    "    #Here I am reading the tweets one by one and process it\n",
    "    inpTweets = tweets_and_sentiments\n",
    "    tweets = []\n",
    "  \n",
    "    for index, row in inpTweets.iterrows():\n",
    "        sentiment = row.is_there_an_emotion_directed_at_a_brand_or_product\n",
    "        tweet = row.text\n",
    "        tweets.append((tweet, sentiment))\n",
    "    #print \"Printing the tweets con su sentiment\"\n",
    "    #print tweets\n",
    "    return tweets #Here I am returning the tweets inside the array plus its sentiment\n",
    "#end\n",
    "\n",
    "#Classifier \n",
    "def get_words_in_tweets(tweets):\n",
    "    all_words = []\n",
    "    for (text, sentiment) in tweets:\n",
    "        all_words.extend(text)\n",
    "    return all_words\n",
    "\n",
    "def get_word_features(wordlist):\n",
    "    \n",
    "    # This line calculates the frequency distrubtion of all words in tweets\n",
    "    wordlist = nltk.FreqDist(wordlist)\n",
    "    word_features = wordlist.keys()\n",
    "    \n",
    "    # This prints out the list of all distinct words in the text in order\n",
    "    # of their number of occurrences.\n",
    "    return word_features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Q5nlCuaaH0OD"
   },
   "source": [
    "## 21. **Predicting the sentiment:**\n",
    "\n",
    "\n",
    "### Use Naive Bayes and Logistic Regression and their accuracy scores for predicting the sentiment of the given text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 430,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2AbVYssaH0OE"
   },
   "outputs": [],
   "source": [
    "tweets = featureExtraction(tweets_final_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 431,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ktXrLhmOH0Of"
   },
   "outputs": [],
   "source": [
    "word_features = get_word_features(get_words_in_tweets(tweets)) #my list of many words "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 433,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "clv2X0kKH0Ok"
   },
   "outputs": [],
   "source": [
    "training_set = nltk.classify.apply_features(extract_features, tweets[:500])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 434,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "K86LRMfdH0Ou"
   },
   "outputs": [],
   "source": [
    "test_set = nltk.classify.apply_features(extract_features, tweets[:250])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 435,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = nltk.NaiveBayesClassifier.train(training_set)\n",
    "\n",
    "# Accuracy\n",
    "accuracy = nltk.classify.accuracy(classifier, training_set) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 436,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.944\n",
      "Naive Bayes Accuracy: %4.2f 94.39999999999999\n"
     ]
    }
   ],
   "source": [
    "#Printing the accuracy\n",
    "print(accuracy) \n",
    "\n",
    "total = accuracy * 100 \n",
    "print('Naive Bayes Accuracy: %4.2f', total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 437,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.952\n",
      "\n",
      "Naive Bayes Accuracy with the Test Set: %4.2f 95.19999999999999\n",
      "\n",
      "Informative features\n",
      "Most Informative Features\n",
      "          contains(dead) = True                0 : 1      =     12.3 : 1.0\n",
      "          contains(busi) = True                0 : 1      =     12.3 : 1.0\n",
      "    contains(blackberri) = True                0 : 1      =     12.3 : 1.0\n",
      "         contains(might) = True                0 : 1      =     12.3 : 1.0\n",
      "          contains(feel) = True                0 : 1      =     12.3 : 1.0\n",
      "           contains(say) = True                0 : 1      =      9.8 : 1.0\n",
      "             contains(1) = True                0 : 1      =      9.5 : 1.0\n",
      "         contains(exist) = True                0 : 1      =      8.8 : 1.0\n",
      "         contains(guess) = True                0 : 1      =      8.8 : 1.0\n",
      "         contains(spend) = True                0 : 1      =      8.8 : 1.0\n",
      "          contains(base) = True                0 : 1      =      8.8 : 1.0\n",
      "        contains(sunday) = True                0 : 1      =      8.8 : 1.0\n",
      "     contains(overheard) = True                0 : 1      =      8.8 : 1.0\n",
      "        contains(upgrad) = True                0 : 1      =      8.8 : 1.0\n",
      "        contains(attent) = True                0 : 1      =      8.8 : 1.0\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# Accuracy Test Set\n",
    "accuracyTestSet = nltk.classify.accuracy(classifier, test_set) \n",
    "\n",
    "#Printing the accuracy for the test set \n",
    "print(accuracyTestSet)\n",
    "\n",
    "totalTest = accuracyTestSet * 100 \n",
    "print('\\nNaive Bayes Accuracy with the Test Set: %4.2f', totalTest)\n",
    "\n",
    "print('\\nInformative features')\n",
    "print(classifier.show_most_informative_features(n=15))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "sw-0B33tH0Ox"
   },
   "source": [
    "## 22. Create a function called `tokenize_predict` which can take count vectorizer object as input and prints the accuracy for x (text) and y (labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 536,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "okCTOs1TH0Oy"
   },
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "def tokenize_test(vect):\n",
    "    x_train_dtm = vect.fit_transform(x_train[0])\n",
    "    print('Features: ', x_train_dtm.shape[1])\n",
    "    x_test_dtm = vect.transform(x_test[0])\n",
    "    nb = MultinomialNB()\n",
    "    nb.fit(x_train_dtm, y_train)\n",
    "    y_pred_class = nb.predict(x_test_dtm)\n",
    "    print('Accuracy: ', metrics.accuracy_score(y_test, y_pred_class))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 537,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_tweets_for_vect(tweet):\n",
    "\n",
    "    tweet = re.sub(\"\\d+\", \"\", tweet)\n",
    "    \n",
    "    # remove stock market tickers like $GE\n",
    "    tweet = re.sub(r'\\$\\w*', '', tweet)\n",
    " \n",
    "    # remove old style retweet text \"RT\"\n",
    "    tweet = re.sub(r'^RT[\\s]+', '', tweet)\n",
    " \n",
    "    # remove hyperlinks\n",
    "    tweet = re.sub(r'https?:\\/\\/.*[\\r\\n]*', '', tweet)\n",
    "    \n",
    "    # remove hashtags\n",
    "    # only removing the hash # sign from the word\n",
    "    tweet = re.sub(r'#', '', tweet)\n",
    " \n",
    "    # tokenize tweets\n",
    "    tokenizer = TweetTokenizer(preserve_case=False, strip_handles=True, reduce_len=True)\n",
    "    tweet_tokens = tokenizer.tokenize(tweet)\n",
    " \n",
    "    tweets_clean = []    \n",
    "    for word in tweet_tokens:\n",
    "        if (word not in stopwords_english and # remove stopwords\n",
    "              word not in emoticons and # remove emoticons\n",
    "                word not in string.punctuation and\n",
    "                   word not in range(0,9)): # remove punctuation\n",
    "            #tweets_clean.append(word)\n",
    "            stem_word = stemmer.stem(word) # stemming word\n",
    "            tweets_clean.append(stem_word)\n",
    "            \n",
    "    tweets_clean = ', '.join(tweets_clean)\n",
    "    return str(tweets_clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 538,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hello, great, day, good, morn\n"
     ]
    }
   ],
   "source": [
    "custom_tweet = \"RT @Twitter @chapagain Hello There! Have a great day. :) #good #morning http://chapagain.com.np\"\n",
    " \n",
    "# print cleaned tweet\n",
    "print (clean_tweets_for_vect(custom_tweet))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 539,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = pd.DataFrame(clean_tweets_for_vect(str(text)) for text in tweets_data[valid_tweets].tweet_text)\n",
    "Y = pd.DataFrame(tweets_data_valid.is_there_an_emotion_directed_at_a_brand_or_product)\n",
    "x_train, x_test, y_train, y_test = train_test_split(X, Y, test_size=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 540,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features:  3870\n",
      "Accuracy:  0.8676056338028169\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/disks/user/anaconda3/lib/python3.6/site-packages/sklearn/utils/validation.py:578: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    }
   ],
   "source": [
    "tokenize_test(CountVectorizer())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "JxZ8jfPEH0O0"
   },
   "source": [
    "### Create a count vectorizer function which includes n_grams = 1,2  and pass it to tokenize_predict function to print the accuracy score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 541,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "kdCyAN_IH0O0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features:  18396\n",
      "Accuracy:  0.8760563380281691\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/disks/user/anaconda3/lib/python3.6/site-packages/sklearn/utils/validation.py:578: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    }
   ],
   "source": [
    "tokenize_test(CountVectorizer(ngram_range = (1,2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "axepytmgH0O4"
   },
   "source": [
    "### Create a count vectorizer function with stopwords = 'english'  and pass it to tokenize_predict function to print the accuracy score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 542,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "HToGkq7vH0O4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features:  3740\n",
      "Accuracy:  0.8685446009389671\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/disks/user/anaconda3/lib/python3.6/site-packages/sklearn/utils/validation.py:578: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    }
   ],
   "source": [
    "tokenize_test(CountVectorizer(stop_words = 'english'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "iOIlJRxoH0O7"
   },
   "source": [
    "### Create a count vectorizer function with stopwords = 'english' and max_features =300  and pass it to tokenize_predict function to print the accuracy score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 543,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6fUhff-oH0O8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features:  300\n",
      "Accuracy:  0.8262910798122066\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/disks/user/anaconda3/lib/python3.6/site-packages/sklearn/utils/validation.py:578: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    }
   ],
   "source": [
    "tokenize_test(CountVectorizer(stop_words = 'english', max_features = 300))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "S2KZNWVkH0PA"
   },
   "source": [
    "### Create a count vectorizer function with n_grams = 1,2  and max_features = 15000  and pass it to tokenize_predict function to print the accuracy score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 545,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3v9XD082H0PB"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features:  15000\n",
      "Accuracy:  0.8732394366197183\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/disks/user/anaconda3/lib/python3.6/site-packages/sklearn/utils/validation.py:578: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    }
   ],
   "source": [
    "tokenize_test(CountVectorizer(ngram_range = (1,2), max_features = 15000))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "We3JK_SRH0PO"
   },
   "source": [
    "### Create a count vectorizer function with n_grams = 1,2  and include terms that appear at least 2 times (min_df = 2)  and pass it to tokenize_predict function to print the accuracy score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 546,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "fUHrfDCyH0PP"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features:  5249\n",
      "Accuracy:  0.8469483568075117\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/disks/user/anaconda3/lib/python3.6/site-packages/sklearn/utils/validation.py:578: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    }
   ],
   "source": [
    "tokenize_test(CountVectorizer(ngram_range = (1,2), min_df = 2))"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "R8_Internal_Lab_Questions.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
